name: pointpillar_lss_lora_32_64_adapter_3
data_dir: "/hd_cache/datasets/DAIR-V2X/cooperative-vehicle-infrastructure"
root_dir: "/data/datasets/DAIR-V2X/cooperative-vehicle-infrastructure/train.json"
validate_dir: "/data/datasets/DAIR-V2X/cooperative-vehicle-infrastructure/val.json"
test_dir: "/data/datasets/DAIR-V2X/cooperative-vehicle-infrastructure/val.json"
# data_dir: "/mimer/NOBACKUP/groups/junjie_storage/datasets/DAIR-V2X/cooperative-vehicle-infrastructure"
# root_dir: "/mimer/NOBACKUP/groups/junjie_storage/datasets/DAIR-V2X/cooperative-vehicle-infrastructure/train.json"
# validate_dir: "/mimer/NOBACKUP/groups/junjie_storage/datasets/DAIR-V2X/cooperative-vehicle-infrastructure/val.json"
# test_dir: "/mimer/NOBACKUP/groups/junjie_storage/datasets/DAIR-V2X/cooperative-vehicle-infrastructure/val.json"

# 这种情况创建supernet去训练, point_pillar是一种配置， second是一种配置
load_history: false
bev_h: &bev_h 100
bev_w: &bev_w 252
train_agent_ID: &train_agent_ID -1  # -1为协同训练，0,1,2为各自的ID； 0是ego, 1是infra

method_v: &method_v point_pillar
method_i: &method_i lss

# method_v_path: ./opencood/logs/v_point_pillar_2024_08_01_17_43_01/net_epoch_bestval_at13.pth
method_ego_path: ./opencood/logs/ego_pointpillar_2024_08_13_13_06_12/net_epoch_bestval_at13.pth
method_i_path: ./opencood/logs/lss_i_2024_08_17_20_18_20/net_epoch_bestval_at29.pth

noise_setting:
  add_noise: false
  args: 
    pos_std: 0
    rot_std: 0
    pos_mean: 0
    rot_mean: 0

yaml_parser: ["load_point_pillar_params", "load_second_params", "load_lift_splat_shoot_params"]
train_params:
  batch_size: &batch_size 4
  epoches: 20
  eval_freq: 2
  save_freq: 2
  max_cav: &max_cav 2

# input_source: ['lidar']
# label_type: 'lidar'
input_source: ['camera', 'lidar']
label_type: 'lidar'

camera_data_aug_conf: &data_aug_conf
        resize_lim: [0.27, 0.28]
        final_dim: [288, 512]  # 
        rot_lim: [0, 0]
        H: 1080   # 原始图片大小
        W: 1920
        rand_flip: False
        bot_pct_lim: [0.0, 0.05]
        cams: ['camera0', 'camera1', 'camera2', 'camera3']
        Ncams: 4

comm_range: 1000

fusion:
  core_method: 'agnostic'
  dataset: 'dairv2x'
  args: 
    proj_first: false

data_augment:
  - NAME: random_world_flip
    ALONG_AXIS_LIST: [ 'x' ]

  - NAME: random_world_rotation
    WORLD_ROT_ANGLE: [ -0.78539816, 0.78539816 ]

  - NAME: random_world_scaling
    WORLD_SCALE_RANGE: [ 0.95, 1.05 ]


# 不同的方法有不同的前处理和后处理
point_pillar:
  # preprocess-related
  preprocess:
    # options: BasePreprocessor, VoxelPreprocessor, BevPreprocessor
    core_method: 'SpVoxelPreprocessor'
    args: 
      voxel_size: &point_pillar_voxel_size [0.4, 0.4, 5]  # 初始大小： 200 * 504 
      max_points_per_voxel: 32
      max_voxel_train: 32000
      max_voxel_test: 70000
    # lidar range for each individual cav.
    cav_lidar_range: &point_pillar_cav_lidar [-100.8, -40, -3.5, 100.8, 40, 1.5]
  
  # anchor box related
  postprocess:
    core_method: 'VoxelPostprocessor' # VoxelPostprocessor, BevPostprocessor supported
    gt_range: *point_pillar_cav_lidar
    anchor_args:
      cav_lidar_range: *point_pillar_cav_lidar
      l: 4.5
      w: 2
      h: 1.56
      r: [0, 90]
      feature_stride: 2
      num: 2
    target_args:
      pos_threshold: 0.6
      neg_threshold: 0.45
      score_threshold: 0.20
    order: 'hwl' # hwl or lwh
    max_num: 100 # maximum number of objects in a single frame. use this number to make sure different frames has the same dimension in the same batch
    nms_thresh: 0.15
    # dir_args: &dir_args
    #   dir_offset: 0.7853
    #   num_bins: 2
    #   anchor_yaw: *point_pillar_anchor_yaw

second:
  # preprocess-related
  preprocess:
    # options: BasePreprocessor, VoxelPreprocessor, BevPreprocessor
    core_method: 'SpVoxelPreprocessor'
    args:
      voxel_size: &second_voxel_size [0.1, 0.1, 0.1]  # 模型的
      max_points_per_voxel: 5
      max_voxel_train: 32000
      max_voxel_test: 70000
    # lidar range for each individual cav. Format: xyzxyz minmax
    cav_lidar_range: &second_cav_lidar [-100.8, -40, -3.5, 100.8, 40, 1.5]
  
  postprocess:
    core_method: 'VoxelPostprocessor' # VoxelPostprocessor, BevPostprocessor supported
    gt_range: *second_cav_lidar
    anchor_args:
      cav_lidar_range: *second_cav_lidar
      l: 4.5
      w: 2
      h: 1.56
      r: [0, 90]
      feature_stride: 8
      num: 2
    target_args:
      pos_threshold: 0.6
      neg_threshold: 0.45
      score_threshold: 0.20
    order: 'hwl' # hwl or lwh
    max_num: 100 # maximum number of objects in a single frame. use this number to make sure different frames has the same dimension in the same batch
    nms_thresh: 0.15
    # dir_args: &dir_args
    #   dir_offset: 0.7853
    #   num_bins: 2
    #   anchor_yaw: *anchor_yaw

lss: 
  preprocess:
    # options: BasePreprocessor, VoxelPreprocessor, BevPreprocessor
    core_method: 'SpVoxelPreprocessor'
    args:
      voxel_size: &lss_voxel_size [0.4, 0.4, 5]
      max_points_per_voxel: 32
      max_voxel_train: 32000
      max_voxel_test: 70000
    # detection range for each individual cav.
    cav_lidar_range: &lss_cav_lidar [-100.8, -40, -3.5, 100.8, 40, 1.5]

  postprocess:
    postprocess:
    core_method: 'VoxelPostprocessor' # That's ok
    gt_range: *lss_cav_lidar
    anchor_args:
      cav_lidar_range: *lss_cav_lidar
      l: 3.9
      w: 1.6
      h: 1.56
      feature_stride: 2
      r: [0, 90]
      num: 2
    target_args:
      pos_threshold: 0.6
      neg_threshold: 0.45
      score_threshold: 0.25
    order: 'hwl' # hwl or lwh
    max_num: 100
    nms_thresh: 0.15
    # dir_args: &dir_args
    #   dir_offset: 0.7853
    #   num_bins: 2
    #   anchor_yaw: *anchor_yaw


# model related
model:
  core_method: model_agnostic_base
  args:
    train_agent_ID: *train_agent_ID
    method_i: *method_i
    method_v: *method_v
    method_fusion: defor_encoder_fusion

    supervise_single: true

    # define which method
    # Method 1: points pillar
    point_pillar:
      voxel_size: *point_pillar_voxel_size
      lidar_range: *point_pillar_cav_lidar
      anchor_number: 2
      # embed_dims: &embed_dims 128 
      multi_scale: true
      head_embed_dims: 384
      pillar_vfe:
        use_norm: true
        with_distance: false
        use_absolute_xyz: true
        num_filters: [64]
      point_pillar_scatter:
        num_features: 64
      base_bev_backbone: # backbone will downsample 2x
        resnet: true
        voxel_size: *point_pillar_voxel_size
        layer_nums: [3, 5, 8]
        layer_strides: [2, 2, 2]
        num_filters: [64, 128, 256]
        upsample_strides: [1, 2, 4]
        num_upsample_filter: [128, 128, 128]
      
    # Method 2: second
    second:
      lidar_range: *second_cav_lidar
      voxel_size: [0.1, 0.1, 0.1]
      mean_vfe:
        num_point_features: 4
      spconv:
        num_features_in: 4
        num_features_out: 64
      map2bev:
        feature_num: 128
      ssfa:
        feature_num: 128
      head:
        num_input: 128
        num_pred: 14
        num_cls: 2
        num_iou: 2
        use_dir: True
        num_dir: 4
      shrink_header:
        kernal_size: [ 3 ]
        stride: [ 1 ]
        padding: [ 1 ]
        dim: [ 128 ]
        input_dim: 128 

    lss:
      anchor_number: 2
      grid_conf: &grid_conf
        # -100.8, -40, -3.5, 100.8, 40, 1.5
        xbound: [-100.8, 100.8, 0.4]   # 需要和preprocess一致. Need to be consistent with preprocess.
        ybound: [-40, 40, 0.4]   # 需要和preprocess一致. Need to be consistent with preprocess.
        zbound: [-10, 10, 20.0]   # 不需要和preprocess一致. NO Need to be consistent with preprocess.
        ddiscr: [2, 100, 98]  # not used
        mode: 'LID' # or 'UD' # not used
      data_aug_conf: *data_aug_conf
      # dir_args: *dir_args
      img_downsample: 8
      img_features: 128
      use_depth_gt: false
      depth_supervision: false
      bevout_feature: 128

      shrink_header:
        kernal_size: [ 3 ]
        stride: [ 2 ]
        padding: [ 1 ]
        dim: [ 128 ]
        input_dim: 128
      camera_encoder: EfficientNet

    # Fusion method
    defor_encoder_fusion: 
      bev_h: *bev_h
      bev_w: *bev_w
      embed_dims: &embed_dims 128 # 128 for multi-scale; 384 for single scale
      max_num_agent: *max_cav
      agent_names: &agent_names ["ego", "lss_v"]
      feature_levels: &feature_levels [3, 1]
      lora_rank: 32

      anchor_number: 2
      # adapter parameters
      n_adapters: 3
      adapters: 
        ego: [[64, 128, 256], [128, 128, 128]]
        # point_pillar: [[64, 128, 256], [128, 128, 128]]
        # second: [[384], [128]]
        lss_v: [[128], [128]]   

      # cross first
      block_cfgs: [[*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 32, 64, ["cross_attn", "norm", "ffn", "norm"]],
                  [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 32, 64, ["cross_attn", "norm", "ffn", "norm"]],
                  [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 32, 64, ["cross_attn", "norm", "ffn", "norm"]],
                  [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 32, 64, ["cross_attn", "norm", "ffn", "norm"]],
                [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 32, 64, ["self_attn", "norm", "ffn", "norm"]],
                [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 32, 64, ["self_attn", "norm", "ffn", "norm"]]]
  
      head_embed_dims: *embed_dims
    
    # defor_encoder_fusion: 
    #   bev_h: *bev_h
    #   bev_w: *bev_w
    #   embed_dims: &embed_dims 128 # 128 for multi-scale; 384 for single scale
    #   max_num_agent: *max_cav
    #   agent_names: &agent_names ["ego", "second"]
    #   feature_levels: &feature_levels [3, 1]
    #   lora_rank: 16
    #   anchor_number: 2
    #   # adapter parameters
    #   adapters: 
    #     ego: [[64, 128, 256], [128, 128, 128]]
    #     # point_pillar: [[64, 128, 256], [128, 128, 128]]
    #     second: [[128], [128]]     

    #   # cross first
    #   block_cfgs: [[*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 16, ["cross_attn", "norm", "ffn", "norm"]],
    #               [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 16, ["cross_attn", "norm", "ffn", "norm"]],
    #               [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 16, ["cross_attn", "norm", "ffn", "norm"]],
    #               [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 16, ["cross_attn", "norm", "ffn", "norm"]],
    #               [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 16, ["self_attn", "norm", "ffn", "norm"]],
    #               [*embed_dims, 8, 8, 8, 8, 0.1, *agent_names, *feature_levels, 16, ["self_attn", "norm", "ffn", "norm"]]]
  
    #   head_embed_dims: *embed_dims
    # # dir_args: *dir_args

loss:
  core_method: point_pillar_loss
  args:
    pos_cls_weight: 2.0
    cls:
      type: 'SigmoidFocalLoss'
      alpha: 0.25
      gamma: 2.0
      weight: 1.0
    reg:
      type: 'WeightedSmoothL1Loss'
      sigma: 3.0
      codewise: true
      weight: 2.0
    # dir:
    #   type: 'WeightedSoftmaxClassificationLoss'
    #   weight: 0.2
    #   args: *dir_args

optimizer:
  core_method: Adam
  lr: 0.002
  args:
    eps: 1e-10
    weight_decay: 1e-4

lr_scheduler:
  core_method: multistep #step, multistep and Exponential support
  gamma: 0.1
  step_size: [10, 15]

